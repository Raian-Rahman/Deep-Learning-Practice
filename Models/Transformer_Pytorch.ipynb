{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Using Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1JjdF0Mn1O6"
      },
      "source": [
        "### Importing the libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72wxTmzqnwtr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import unsqueeze\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        \"\"\"\n",
        "        We are going to split the embed size between these heads. If we have 256 size embedding and 8 heads then we will have 32 embed size for each embedding\n",
        "        \"\"\"\n",
        "        super(SelfAttention,self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        \n",
        "        assert self.embed_size%self.heads == 0 , \"To make sure that embed size is properly divisible by heads\"\n",
        "        \n",
        "        self.head_dim = embed_size//heads\n",
        "\n",
        "        \"\"\"\n",
        "        Now we are defining the Query value and key vectors as Linear layers. \n",
        "        We are setting bias = False, because we dont need that\n",
        "        \"\"\"\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, queries, mask):\n",
        "        N = queries.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "\n",
        "        \"\"\"\n",
        "        split embedding into self.heads pieces\n",
        "        \"\"\"\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        #step 1: multiply query and key\n",
        "        \n",
        "        # queries shape : (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy shape: (N, heads,query_len = target source sentence, key_len = source sentence)\n",
        "        \"\"\"\n",
        "        As we have a batch matrix multiplier einsum is quite handy for it \n",
        "        \"\"\"\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries,keys]) #it is used for matrix multiplication where we have several other dimensions \n",
        "        \n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask==0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy/(self.embed_size**(1/2)),dim=3)\n",
        "\n",
        "\n",
        "        #attention shape: (N,heads, query_len, key_len)\n",
        "        #value shape: (N, value_len, heads, heads_dim)\n",
        "        #out shape: (N, Query_len, heads, heads_dim)\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention,values])\n",
        "\n",
        "\n",
        "        #concatanation part \n",
        "        out  = out.reshape(N,query_len, self.heads*self.head_dim)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxSmB2z_vhF"
      },
      "source": [
        "### Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUzt6d5_uI0l"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    embedding -> multiheaded_attention -> add&norm -> feed forward -> add&norm \n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock,self).__init__()\n",
        "        self.attention = SelfAttention(embed_size=embed_size, heads = heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size) #layernorm and batchnorm are almost similar...but layer norm has more computation\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion*embed_size,embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        \"\"\"\n",
        "        we needed a skip connection. query is for the skip connection\n",
        "        \"\"\"\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward+x))\n",
        "        print(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7o7oW8bBLh9"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
        "        \"\"\"\n",
        "        Encoder block takes a lot of parameters due to hyperparameter. The parameters are explained below:\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        src_vocab_size = size of source vocabulary \n",
        "        embed_size  = dimension of embedding \n",
        "        num_layers = number of transformer layer in encoder\n",
        "        heads = number of heads in multiheads \n",
        "        device = the device on which we want to train\n",
        "        forward_expansion  = the ratio by which we want to expand the size\n",
        "        dropout  = dropout probability\n",
        "        max_length = max sentence length. \n",
        "        maximum length of string to ensure positional embedding which is requeired for ensuring we have attention. \n",
        "        What transformer does is we wnat to ensure that some sort of sequence is maintained even is the layer does not have any recurrent unit. It helps the transformer for ensuring parallelization\n",
        "        \"\"\"\n",
        "        super(Encoder,self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [ \n",
        "              TransformerBlock(embed_size=embed_size, heads=heads, dropout=dropout, forward_expansion=forward_expansion)  for _ in range(num_layers)\n",
        "            ] \n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        \n",
        "        N, seq_length = x.shape\n",
        "\n",
        "        positions = torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "\n",
        "        out = self.dropout(self.word_embedding(x)+self.positional_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out,out,out,mask)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LutflV_FRrR"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock,self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        Decoder block takes a lot of parameters. The parameters are explained below:\n",
        "        ----------------------------------------------------------------------------\n",
        "        x : input \n",
        "        value, key : for self_attention\n",
        "        src_mask: source mask. Although it is optional still we need it. For example, let we have more than one example in the input. In those cases src_mask is needed to make all the sentences equal also we dont need to to extra computations for the masks that are padded\n",
        "        trg_mask: trg_mask is required to make sure that everything works fine\n",
        "        \"\"\"\n",
        "        attention = self.attention(x,x,x,trg_mask)\n",
        "        query = self.dropout(self.norm(attention+x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-_ZuWpYQwC"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.positional_embedding = nn.Embedding(max_length,embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderBlock(embed_size, heads, forward_expansion, dropout, device) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "        x = self.dropout(self.word_embedding(x)+self.positional_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkWj30n7ejgp"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size = 256, num_layers = 6, forward_expansion = 4, heads = 8, dropout = 0, device = \"cuda\", max_length = 100):\n",
        "        super(Transformer,self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size,embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
        "        self.decoder = Decoder(trg_vocab_size,embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def make_src_mask(self,src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self,trg):\n",
        "        N, trg_len = trg.shape\n",
        "\n",
        "        trg_mask = torch.tril(torch.ones((trg_len,trg_len))).expand(N,1,trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx1y1NMnhORX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6ab23a59-86a4-4b37-c082-a121c4419948"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    x = torch.tensor([[1,5,6,4,3,9,5,2,0],[1,8,7,3,4,5,6,7,2]]).to(device)\n",
        "    trg = torch.tensor([[1,7,4,3,5,9,2,0],[1,8,7,3,4,5,6,2]]).to(device)\n",
        "\n",
        "    src_pad_idx = 0\n",
        "\n",
        "    trg_pad_idx = 0\n",
        "    src_vocab_size = 10\n",
        "    trg_vocab_Size = 10\n",
        "\n",
        "    model = Transformer(src_vocab_size, trg_vocab_Size, src_pad_idx, trg_pad_idx).to(device)\n",
        "\n",
        "    out = model(x,trg[:,:-1])\n",
        "    print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fe0549bfac58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab_Size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5a05fdd6a673>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_src_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5a05fdd6a673>\u001b[0m in \u001b[0;36mmake_src_mask\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_src_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    }
  ]
}